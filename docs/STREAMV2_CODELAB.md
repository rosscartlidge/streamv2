# StreamV2 Codelab: From Zero to Stream Processing Hero

Welcome to the StreamV2 codelab! This hands-on tutorial will teach you modern stream processing in Go using a simple, elegant API. You'll start with basic concepts and gradually build up to powerful data processing pipelines.

## What You'll Learn

- âœ… Core stream operations (Map, Filter, Reduce)
- âœ… Data aggregation and statistics
- âœ… Working with CSV, JSON, and other formats
- âœ… Building processing pipelines
- âœ… Real-world data analysis patterns

ðŸ“š **Advanced Topics**: For complex windowing, event-time processing, and real-time analytics, see the [Advanced Windowing Codelab](ADVANCED_WINDOWING_CODELAB.md)

## Tutorial Navigation

**Beginner:** [Step 1](#step-1-your-first-stream) â€¢ [Step 2](#step-2-transform-data-with-map) â€¢ [Step 3](#step-3-filter-data-with-where) â€¢ [Step 4](#step-4-combine-operations-with-pipe)

**Intermediate:** [Step 5](#step-5-aggregate-data) â€¢ [Step 6](#step-6-working-with-real-data---csv-processing) â€¢ [Step 7](#step-7-data-analysis-pipeline) â€¢ [Step 8](#step-8-working-with-json-data)

**Advanced:** [Step 9](#step-9-join-operations---combining-data-streams) â€¢ [Step 10](#step-10-sorting-your-data) â€¢ [Step 11](#step-11-advanced-stream-operations) â€¢ [Step 12](#step-12-best-practices-and-performance-tips)

## Prerequisites

- Basic Go knowledge
- Go 1.21+ installed

---

## Setup

```bash
# Clone the repository
git clone https://github.com/rosscartlidge/streamv2
cd streamv2

# Run the examples
go run examples/codelab/step1.go
```

---

# Step 1: Your First Stream

## What Are Streams?

Think of a stream as a **pipeline of data** that flows through your program. Instead of processing all your data at once in memory, streams let you process data **one piece at a time**, which is:

- **Memory efficient**: Handle millions of records without loading them all at once
- **Composable**: Chain operations together like building blocks
- **Lazy**: Nothing happens until you actually need the results

## The Stream Mindset

Traditional Go:
```go
// Process everything at once
numbers := []int{1, 2, 3, 4, 5}
doubled := make([]int, len(numbers))
for i, n := range numbers {
    doubled[i] = n * 2  // All in memory
}
```

StreamV2 Way:
```go
// Build a processing pipeline
numbers := []int{1, 2, 3, 4, 5}
doubled := stream.Map(func(n int) int { return n * 2 })(
    stream.FromSlice(numbers)
)
// Nothing processed yet! Just a pipeline definition.
```

## Your First Stream

Let's start with the simplest possible example - converting data to a stream and back:

```go
package main

import (
    "fmt"
    "github.com/rosscartlidge/streamv2/pkg/stream"
)

func main() {
    fmt.Println("ðŸš€ Welcome to StreamV2!")

    // Step 1: Start with regular Go data
    numbers := []int64{1, 2, 3, 4, 5}
    fmt.Println("ðŸ“Š Original data:", numbers)

    // Step 2: Convert to a stream (creates a pipeline)
    numberStream := stream.FromSlice(numbers)
    fmt.Println("ðŸ”„ Created stream (no processing yet!)")

    // Step 3: Collect results (this triggers processing)
    result, err := stream.Collect(numberStream)
    if err != nil {
        fmt.Printf("âŒ Error: %v\n", err)
        return
    }

    fmt.Println("âœ… Stream result:", result)
    fmt.Println("ðŸ’¡ Notice: Same data, but it went through a stream!")
}
```

## What Just Happened?

1. **`FromSlice(numbers)`**: Converts your regular Go slice into a stream
   - Think of this as "putting your data on a conveyor belt"
   - No processing happens yet - it's just preparation

2. **`Collect(numberStream)`**: Pulls all data through the stream
   - This is when processing actually happens
   - Converts the stream back to a regular Go slice

3. **Lazy Evaluation**: The magic of streams!
   - You build the pipeline first (`FromSlice`)
   - Processing only happens when you ask for results (`Collect`)

## Why This Matters

This might seem pointless (same input, same output), but you've just learned the **fundamental pattern** of stream processing:

```
Data â†’ Stream â†’ Operations â†’ Collect â†’ Results
```

Every StreamV2 program follows this pattern. Next, we'll add the "Operations" part to make it useful!

## Try It Yourself

Run the code above and observe:
- âœ… You get the same data back
- âœ… No errors or crashes
- âœ… You've successfully used streams!

This foundation will support everything else we build together.

---

# Step 2: Transform Data with Map

## The Power of Transformation

Now comes the fun part! `Map` is like having a **magical transformation machine**. You put data in, apply a function to each piece, and get transformed data out.

Think of it like an assembly line:
- **Input**: Raw materials (your data)
- **Machine**: Your transformation function
- **Output**: Finished products (transformed data)

## Real-World Analogy

Imagine you run a bakery:
- **Input**: `[flour, sugar, eggs, butter]`
- **Transform**: "bake into" function
- **Output**: `[bread, cake, cookies, pastry]`

That's exactly what `Map` does, but with data!

## Your First Transformation

Let's start with a simple math example:

```go
package main

import (
    "fmt"
    "github.com/rosscartlidge/streamv2/pkg/stream"
)

func main() {
    fmt.Println("ðŸ”¢ Learning Map - Data Transformation!")

    // Our input data - simple numbers
    numbers := []int64{1, 2, 3, 4, 5}
    fmt.Println("ðŸ“Š Original numbers:", numbers)

    // Transform: Square each number
    fmt.Println("ðŸ”„ Squaring each number...")
    squares, err := stream.Collect(
        stream.Map(func(x int64) int64 {
            fmt.Printf("   Transforming %d â†’ %d\n", x, x*x)
            return x * x
        })(stream.FromSlice(numbers)))

    if err != nil {
        fmt.Printf("âŒ Error: %v\n", err)
        return
    }

    fmt.Println("âœ… Squares:", squares)
    fmt.Println()

    // Transform: Numbers to labels (change type!)
    fmt.Println("ðŸ·ï¸  Converting numbers to labels...")
    labels, err := stream.Collect(
        stream.Map(func(x int64) string {
            label := fmt.Sprintf("Item-%d", x)
            fmt.Printf("   Converting %d â†’ %s\n", x, label)
            return label
        })(stream.FromSlice(numbers)))

    if err != nil {
        fmt.Printf("âŒ Error: %v\n", err)
        return
    }

    fmt.Println("âœ… Labels:", labels)
}
```

## Breaking Down the Magic

Let's understand what's happening step by step:

### 1. The Map Function
```go
stream.Map(func(x int64) int64 {
    return x * x
})
```

This creates a transformation that says: "For each number `x`, return `x * x`"

### 2. The Pipeline Pattern
```go
stream.Map(...)(stream.FromSlice(numbers))
```

This might look weird at first! It's StreamV2's way of chaining operations:
- `stream.Map(...)` creates a transformation function
- `(stream.FromSlice(numbers))` applies it to your data stream

Think of it as: `transformation(data)`

### 3. Type Changes
```go
stream.Map(func(x int64) string {
    return fmt.Sprintf("Item-%d", x)
})
```

Notice how we go from `int64` â†’ `string`. Map can change types completely!

## What You'll See

When you run this code:
```
ðŸ”¢ Learning Map - Data Transformation!
ðŸ“Š Original numbers: [1 2 3 4 5]
ðŸ”„ Squaring each number...
   Transforming 1 â†’ 1
   Transforming 2 â†’ 4
   Transforming 3 â†’ 9
   Transforming 4 â†’ 16
   Transforming 5 â†’ 25
âœ… Squares: [1 4 9 16 25]

ðŸ·ï¸  Converting numbers to labels...
   Converting 1 â†’ Item-1
   Converting 2 â†’ Item-2
   Converting 3 â†’ Item-3
   Converting 4 â†’ Item-4
   Converting 5 â†’ Item-5
âœ… Labels: [Item-1 Item-2 Item-3 Item-4 Item-5]
```

## Key Insights

1. **One-to-One**: Map always produces the same number of outputs as inputs
   - 5 numbers in â†’ 5 numbers out
   - 5 numbers in â†’ 5 strings out

2. **Type Safety**: Go's type system ensures your transformations are correct
   - If your function expects `int64`, you can't accidentally pass `string`

3. **Pure Functions**: Your transformation function should be predictable
   - Same input â†’ same output (every time)
   - No side effects (don't modify global variables)

## Common Use Cases

- **Math transformations**: Squaring, doubling, converting units
- **String formatting**: Adding prefixes, changing case, templating
- **Data extraction**: Getting fields from complex objects
- **Type conversion**: Numbers to strings, strings to numbers

You've just learned the most fundamental stream operation! Next, we'll learn how to filter data to keep only what we want.

---

# Step 3: Filter Data with Where

## The Art of Selection

While `Map` transforms data, `Where` **selects** data. Think of it as having a **quality control inspector** on your assembly line who decides which items to keep and which to reject.

## Real-World Analogy

Imagine sorting fruit at a grocery store:
- **Input**: `[apple, orange, rotten_apple, banana, moldy_orange]`
- **Filter**: "Keep only fresh fruit"
- **Output**: `[apple, orange, banana]`

That's exactly what `Where` does - it applies a **test** to each item and keeps only those that pass!

## Your First Filter

Let's start with a simple example - finding even numbers:

```go
package main

import (
    "fmt"
    "github.com/rosscartlidge/streamv2/pkg/stream"
)

func main() {
    fmt.Println("ðŸ” Learning Where - Data Filtering!")

    // Our test data - mixed numbers
    numbers := []int64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
    fmt.Println("ðŸ“Š Original numbers:", numbers)

    // Filter: Keep only even numbers
    fmt.Println("ðŸ”„ Testing each number for evenness...")
    evens, err := stream.Collect(
        stream.Where(func(x int64) bool {
            isEven := x%2 == 0
            fmt.Printf("   Testing %d: even? %v â†’ ", x, isEven)
            if isEven {
                fmt.Println("KEEP âœ…")
            } else {
                fmt.Println("DISCARD âŒ")
            }
            return isEven
        })(stream.FromSlice(numbers)))

    if err != nil {
        fmt.Printf("âŒ Error: %v\n", err)
        return
    }

    fmt.Println("âœ… Even numbers:", evens)
    fmt.Println()

    // Filter: Keep only large numbers (> 5)
    fmt.Println("ðŸ”„ Testing each number for size...")
    bigNumbers, err := stream.Collect(
        stream.Where(func(x int64) bool {
            isBig := x > 5
            fmt.Printf("   Testing %d > 5: %v â†’ ", x, isBig)
            if isBig {
                fmt.Println("KEEP âœ…")
            } else {
                fmt.Println("DISCARD âŒ")
            }
            return isBig
        })(stream.FromSlice(numbers)))

    if err != nil {
        fmt.Printf("âŒ Error: %v\n", err)
        return
    }

    fmt.Println("âœ… Numbers > 5:", bigNumbers)
}
```

## Understanding the Filter Function

### The Predicate Function
```go
stream.Where(func(x int64) bool {
    return x%2 == 0  // This is your "test"
})
```

This function is called a **predicate** - it asks a yes/no question about each element:
- **Input**: One element from your stream
- **Output**: `true` (keep it) or `false` (discard it)

### Common Predicate Patterns

```go
// Numeric comparisons
stream.Where(func(x int64) bool { return x > 10 })     // Greater than
stream.Where(func(x int64) bool { return x <= 5 })     // Less than or equal
stream.Where(func(x int64) bool { return x == 42 })    // Exactly equal

// Mathematical tests
stream.Where(func(x int64) bool { return x%3 == 0 })   // Divisible by 3
stream.Where(func(x int64) bool { return x*x < 100 })  // Square less than 100

// Range tests
stream.Where(func(x int64) bool { return x >= 10 && x <= 20 }) // Between 10 and 20
```

## What You'll See

When you run the code:
```
ðŸ” Learning Where - Data Filtering!
ðŸ“Š Original numbers: [1 2 3 4 5 6 7 8 9 10]
ðŸ”„ Testing each number for evenness...
   Testing 1: even? false â†’ DISCARD âŒ
   Testing 2: even? true â†’ KEEP âœ…
   Testing 3: even? false â†’ DISCARD âŒ
   Testing 4: even? true â†’ KEEP âœ…
   Testing 5: even? false â†’ DISCARD âŒ
   Testing 6: even? true â†’ KEEP âœ…
   Testing 7: even? false â†’ DISCARD âŒ
   Testing 8: even? true â†’ KEEP âœ…
   Testing 9: even? false â†’ DISCARD âŒ
   Testing 10: even? true â†’ KEEP âœ…
âœ… Even numbers: [2 4 6 8 10]

ðŸ”„ Testing each number for size...
   Testing 1 > 5: false â†’ DISCARD âŒ
   Testing 2 > 5: false â†’ DISCARD âŒ
   Testing 3 > 5: false â†’ DISCARD âŒ
   Testing 4 > 5: false â†’ DISCARD âŒ
   Testing 5 > 5: false â†’ DISCARD âŒ
   Testing 6 > 5: true â†’ KEEP âœ…
   Testing 7 > 5: true â†’ KEEP âœ…
   Testing 8 > 5: true â†’ KEEP âœ…
   Testing 9 > 5: true â†’ KEEP âœ…
   Testing 10 > 5: true â†’ KEEP âœ…
âœ… Numbers > 5: [6 7 8 9 10]
```

## Key Insights

1. **Order Preserved**: Filtered elements keep their original order
   - `[1,2,3,4,5,6,7,8,9,10]` â†’ `[2,4,6,8,10]` (evens stay in order)

2. **Size Reduction**: Filtering usually makes your data smaller
   - 10 numbers â†’ 5 even numbers
   - This saves memory and processing time

3. **All or Nothing**: Each element either passes or fails completely
   - No "partial" filtering - it's binary

4. **Pure Predicates**: Your test function should be predictable
   - Same input â†’ same result (every time)
   - No side effects (don't modify data inside the test)

## Common Use Cases

- **Data cleaning**: Remove invalid or corrupt records
- **Search results**: Find items matching criteria
- **Validation**: Keep only items that pass quality checks
- **Security**: Filter out unauthorized requests
- **Performance**: Process only relevant data

## The Power of Filtering

Filtering is incredibly powerful because it:
- **Reduces noise**: Focus on what matters
- **Improves performance**: Process less data
- **Enables precision**: Target exactly what you need

Next, we'll learn how to combine Map and Where together for even more powerful data processing!

---

# Step 4: Combine Operations with Pipe

## Building Data Processing Pipelines

Now comes the **real magic**! You've learned `Map` (transform) and `Where` (filter) individually. But the true power of streams comes from **combining** operations to build sophisticated data processing pipelines.

## The Assembly Line Concept

Think of it like a car manufacturing assembly line:
1. **Station 1**: Install engine (transform)
2. **Station 2**: Quality check (filter)
3. **Station 3**: Paint car (transform)
4. **Station 4**: Final inspection (filter)

Each station does one job, but together they create something amazing!

## Your First Pipeline

Let's build a pipeline that finds the "big squares" - numbers that become large when squared:

```go
package main

import (
    "fmt"
    "github.com/rosscartlidge/streamv2/pkg/stream"
)

func main() {
    fmt.Println("ðŸ”§ Building Your First Pipeline!")

    numbers := []int64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
    fmt.Println("ðŸ“Š Starting with:", numbers)

    // BUILD THE PIPELINE
    fmt.Println("\nðŸ—ï¸  Building pipeline: Square â†’ Filter(>20)")

    // Method 1: Using Pipe (clean and readable)
    result, err := stream.Collect(
        stream.Pipe(
            stream.Map(func(x int64) int64 {
                fmt.Printf("   ðŸ”¢ Squaring %d â†’ %d\n", x, x*x)
                return x * x
            }),
            stream.Where(func(x int64) bool {
                keep := x > 20
                fmt.Printf("   ðŸ” Testing %d > 20: %v â†’ ", x, keep)
                if keep {
                    fmt.Println("KEEP âœ…")
                } else {
                    fmt.Println("DISCARD âŒ")
                }
                return keep
            }),
        )(stream.FromSlice(numbers)))

    if err != nil {
        fmt.Printf("âŒ Error: %v\n", err)
        return
    }

    fmt.Printf("\nâœ… Final result: %v\n", result)

    // STEP-BY-STEP ANALYSIS
    fmt.Println("\nðŸ“‹ Let's trace this step by step:")

    // Step 1: Just the squares
    squares, _ := stream.Collect(
        stream.Map(func(x int64) int64 { return x * x })(
            stream.FromSlice(numbers)))

    fmt.Printf("   Step 1 (squares): %v\n", squares)
    fmt.Printf("   Step 2 (filter >20): %v\n", result)

    // ALTERNATIVE: Manual chaining (harder to read)
    fmt.Println("\nðŸ”— Alternative: Manual chaining")
    manualResult, _ := stream.Collect(
        stream.Where(func(x int64) bool { return x > 20 })(
            stream.Map(func(x int64) int64 { return x * x })(
                stream.FromSlice(numbers))))

    fmt.Printf("   Same result: %v\n", manualResult)
}
```

## Understanding Pipeline Flow

Let's trace exactly what happens to each number:

```
Input: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

Pipeline: Square â†’ Filter(>20)

Number 1: 1 â†’ 1Â² = 1  â†’ 1 > 20? NO  â†’ DISCARD
Number 2: 2 â†’ 2Â² = 4  â†’ 4 > 20? NO  â†’ DISCARD
Number 3: 3 â†’ 3Â² = 9  â†’ 9 > 20? NO  â†’ DISCARD
Number 4: 4 â†’ 4Â² = 16 â†’ 16 > 20? NO â†’ DISCARD
Number 5: 5 â†’ 5Â² = 25 â†’ 25 > 20? YES â†’ KEEP
Number 6: 6 â†’ 6Â² = 36 â†’ 36 > 20? YES â†’ KEEP
...and so on...

Final Output: [25, 36, 49, 64, 81, 100]
```

## Even Simpler with Chain

For operations on the **same data type**, there's an even simpler way using `Chain`:

```go
package main

import (
    "fmt"
    "github.com/rosscartlidge/streamv2/pkg/stream"
)

func main() {
    numbers := []int{-5, -2, 1, 3, 7, 10, 15, 20}

    fmt.Printf("ðŸ”— Chain Example: Clean up and double positive numbers\n")
    fmt.Printf("Starting with: %v\n\n", numbers)

    result, _ := stream.Collect(
        stream.Chain(
            // Step 1: Keep only positive numbers
            stream.Where(func(x int) bool { return x > 0 }),
            // Step 2: Double them
            stream.Map(func(x int) int { return x * 2 }),
            // Step 3: Keep only those less than 30
            stream.Where(func(x int) bool { return x < 30 }),
            // Step 4: Add 1 to each
            stream.Map(func(x int) int { return x + 1 }),
        )(stream.FromSlice(numbers)))

    fmt.Printf("Final result: %v\n", result)
    // Output: [3, 7, 15, 21] (1â†’2â†’3, 3â†’6â†’7, 7â†’14â†’15, 10â†’20â†’21)
}
```

**Why Chain is Great:**
- âœ… **Same type simplicity** - all operations work on the same type (in this case, `int`)
- âœ… **Super readable** - just list operations in order
- âœ… **Easy to modify** - add/remove steps easily
- âœ… **Incredibly powerful with Records** - chain multiple data transformations, filters, and enrichments on structured data
- âœ… **Perfect for data processing** pipelines

## Three Ways to Build Pipelines

### Method 1: Using `Chain` (Simplest for same-type operations)
```go
stream.Chain(
    stream.Where(func(x int) bool { return x > 0 }),
    stream.Map(func(x int) int { return x * x }),
    stream.Where(func(x int) bool { return x > 20 }),
    stream.Map(func(x int) int { return x + 1 }),
)(stream.FromSlice(numbers))
```

**Perfect for**:
- âœ… Multiple operations on the same data type (int, Record, etc.)
- âœ… Super simple - no type juggling
- âœ… Clean, readable pipeline
- âœ… Powerful Record processing - filter, transform, enrich data

### Chain() with Records - The Real Power!

Here's where Chain() truly shines - processing structured data through multiple transformations:

```go
package main

import (
    "fmt"
    "github.com/rosscartlidge/streamv2/pkg/stream"
)

func main() {
    fmt.Println("ðŸ’ª Chain() with Records - The Real Power!")

    // Sample employee data using fluent builder interface
    employees := []stream.Record{
        stream.NewRecord().String("name", "Alice").Int("salary", 50000).String("department", "Engineering").Build(),
        stream.NewRecord().String("name", "Bob").Int("salary", 35000).String("department", "Marketing").Build(),
        stream.NewRecord().String("name", "Charlie").Int("salary", 75000).String("department", "Engineering").Build(),
        stream.NewRecord().String("name", "Diana").Int("salary", 42000).String("department", "Sales").Build(),
        stream.NewRecord().String("name", "Eve").Int("salary", 65000).String("department", "Engineering").Build(),
    }

    fmt.Println("ðŸ“Š Processing employee data through Chain()...")

    // Build a powerful Record processing pipeline
    result, err := stream.Collect(
        stream.Chain(
            // Step 1: Filter to Engineering department only
            stream.Where(func(r stream.Record) bool {
                dept, _ := r["department"].AsString()
                return dept == "Engineering"
            }),

            // Step 2: Add a bonus field (10% of salary)
            stream.Map(func(r stream.Record) stream.Record {
                name, _ := r["name"].AsString()
                salary, _ := r["salary"].AsInt()
                dept, _ := r["department"].AsString()
                bonus := salary / 10

                fmt.Printf("   ðŸ’° Added bonus to %s: $%d\n", name, bonus)

                // Create new record with bonus field using fluent builder
                return stream.NewRecord().
                    String("name", name).
                    Int("salary", salary).
                    String("department", dept).
                    Int("bonus", bonus).
                    Build()
            }),

            // Step 3: Filter to high earners (salary > $60k)
            stream.Where(func(r stream.Record) bool {
                salary, _ := r["salary"].AsInt()
                return salary > 60000
            }),

            // Step 4: Add total compensation field
            stream.Map(func(r stream.Record) stream.Record {
                name, _ := r["name"].AsString()
                salary, _ := r["salary"].AsInt()
                dept, _ := r["department"].AsString()
                bonus, _ := r["bonus"].AsInt()
                total := salary + bonus

                fmt.Printf("   ðŸ“ˆ Total comp for %s: $%d\n", name, total)

                // Create new record with total compensation using fluent builder
                return stream.NewRecord().
                    String("name", name).
                    Int("salary", salary).
                    String("department", dept).
                    Int("bonus", bonus).
                    Int("total_comp", total).
                    Build()
            }),
        )(stream.FromSlice(employees)))

    if err != nil {
        fmt.Printf("âŒ Error: %v\n", err)
        return
    }

    fmt.Println("\nâœ… High-earning Engineering employees with bonuses:")
    for _, emp := range result {
        name, _ := emp["name"].AsString()
        salary, _ := emp["salary"].AsInt()
        bonus, _ := emp["bonus"].AsInt()
        total, _ := emp["total_comp"].AsInt()
        fmt.Printf("   ðŸ‘¨â€ðŸ’» %s: $%d salary + $%d bonus = $%d total\n", name, salary, bonus, total)
    }
}
```

**This Chain() pipeline:**
1. **Filters** to Engineering department only
2. **Enriches** each record with a calculated bonus field
3. **Filters** to high earners only
4. **Enriches** each record with total compensation

**Why Chain() is perfect for this:**
- âœ… **Same type throughout** - always working with `Record`
- âœ… **Clean pipeline** - easy to read and understand
- âœ… **Easy to modify** - add/remove/reorder steps
- âœ… **Powerful transformations** - filter, enrich, calculate on structured data

### Method 2: Using `Pipe` (For type transformations)
```go
stream.Pipe(
    stream.Map(func(x int64) int64 { return x * x }),
    stream.Where(func(x int64) bool { return x > 20 }),
)(stream.FromSlice(numbers))
```

**Perfect for**:
- âœ… When you need type transformations
- âœ… Easy to read left-to-right
- âœ… Clear operation sequence

### Method 3: Manual Chaining
```go
stream.Where(func(x int64) bool { return x > 20 })(
    stream.Map(func(x int64) int64 { return x * x })(
        stream.FromSlice(numbers)))
```

**Disadvantages**:
- âŒ Reads right-to-left (confusing)
- âŒ Hard to modify
- âŒ Nested parentheses get messy

## Key Pipeline Concepts

1. **Sequential Processing**: Operations happen in order
   - First ALL numbers get squared
   - Then ALL squares get filtered
   - (Not one-by-one, but conceptually in sequence)

2. **Type Safety**: Each step must match the next
   - `Map(int64â†’int64)` â†’ `Where(int64â†’bool)` âœ…
   - `Map(int64â†’string)` â†’ `Where(int64â†’bool)` âŒ (Type mismatch!)

3. **Efficient**: StreamV2 optimizes the pipeline
   - No intermediate arrays created
   - Memory efficient processing

## What You'll See

When you run the code:
```
ðŸ”§ Building Your First Pipeline!
ðŸ“Š Starting with: [1 2 3 4 5 6 7 8 9 10]

ðŸ—ï¸  Building pipeline: Square â†’ Filter(>20)
   ðŸ”¢ Squaring 1 â†’ 1
   ðŸ” Testing 1 > 20: false â†’ DISCARD âŒ
   ðŸ”¢ Squaring 2 â†’ 4
   ðŸ” Testing 4 > 20: false â†’ DISCARD âŒ
   ðŸ”¢ Squaring 3 â†’ 9
   ðŸ” Testing 9 > 20: false â†’ DISCARD âŒ
   ðŸ”¢ Squaring 4 â†’ 16
   ðŸ” Testing 16 > 20: false â†’ DISCARD âŒ
   ðŸ”¢ Squaring 5 â†’ 25
   ðŸ” Testing 25 > 20: true â†’ KEEP âœ…
   ðŸ”¢ Squaring 6 â†’ 36
   ðŸ” Testing 36 > 20: true â†’ KEEP âœ…
   ...

âœ… Final result: [25 36 49 64 81 100]

ðŸ“‹ Let's trace this step by step:
   Step 1 (squares): [1 4 9 16 25 36 49 64 81 100]
   Step 2 (filter >20): [25 36 49 64 81 100]
```

## Pipeline Best Practices

1. **Start Simple**: Begin with 2-3 operations, add more as needed
2. **Use Pipe**: Much more readable than manual chaining
3. **Name Operations**: Create variables for complex functions
4. **Test Each Step**: Verify intermediate results during development

You've now learned the foundation of stream processing! Next, we'll learn about aggregation - turning streams of data into single summary values.

---

# Step 5: Aggregate Data

## From Many to One: The Power of Aggregation

So far you've learned to transform streams (Map) and filter streams (Where). Now let's learn **aggregation** - taking a stream of many values and computing a single summary result.

## Real-World Analogy

Think of aggregation like a **teacher grading exams**:
- **Input**: Stack of test papers `[85, 92, 78, 95, 88]`
- **Aggregations**:
  - Average score: `87.6`
  - Highest score: `95`
  - Lowest score: `78`
  - Total points: `438`
  - Number of students: `5`

That's exactly what stream aggregation does - it **summarizes** your data!

## Understanding Aggregation Types

There are two main categories:

### 1. **Mathematical Aggregations**
- `Sum`: Add all values together
- `Avg`: Calculate the average (mean)
- `Count`: How many items exist

### 2. **Comparison Aggregations**
- `Max`: Find the largest value
- `Min`: Find the smallest value

## Your First Aggregation

Let's analyze some sales data:

```go
package main

import (
    "fmt"
    "github.com/rosscartlidge/streamv2/pkg/stream"
)

func main() {
    fmt.Println("ðŸ“Š Learning Aggregation - Sales Data Analysis!")

    // Daily sales figures for a week (in dollars)
    dailySales := []int64{1200, 1800, 950, 2100, 1650, 2400, 1900}
    fmt.Println("ðŸ“ˆ Daily sales this week:", dailySales)

    fmt.Println("\nðŸ§® Calculating business metrics...")

    // Calculate total revenue
    totalRevenue, err := stream.Sum(stream.FromSlice(dailySales))
    if err != nil {
        fmt.Printf("âŒ Error calculating sum: %v\n", err)
        return
    }
    fmt.Printf("   ðŸ’° Total weekly revenue: $%d\n", totalRevenue)

    // Count number of days
    dayCount, err := stream.Count(stream.FromSlice(dailySales))
    if err != nil {
        fmt.Printf("âŒ Error counting days: %v\n", err)
        return
    }
    fmt.Printf("   ðŸ“… Number of business days: %d\n", dayCount)

    // Calculate average daily sales
    avgSales, err := stream.Avg(stream.FromSlice(dailySales))
    if err != nil {
        fmt.Printf("âŒ Error calculating average: %v\n", err)
        return
    }
    fmt.Printf("   ðŸ“Š Average daily sales: $%.0f\n", avgSales)

    // Find best day
    bestDay, err := stream.Max(stream.FromSlice(dailySales))
    if err != nil {
        fmt.Printf("âŒ Error finding maximum: %v\n", err)
        return
    }
    fmt.Printf("   ðŸ† Best sales day: $%d\n", bestDay)

    // Find worst day
    worstDay, err := stream.Min(stream.FromSlice(dailySales))
    if err != nil {
        fmt.Printf("âŒ Error finding minimum: %v\n", err)
        return
    }
    fmt.Printf("   ðŸ“‰ Lowest sales day: $%d\n", worstDay)

    // ADVANCED: Combine with transformations
    fmt.Println("\nðŸ”„ Advanced: Aggregating after transformation...")

    // Question: What if we gave a 10% bonus on each day?
    bonusTotal, err := stream.Sum(
        stream.Map(func(sales int64) int64 {
            bonus := sales + (sales / 10) // Add 10% bonus
            fmt.Printf("   ðŸ’¡ $%d + 10%% bonus = $%d\n", sales, bonus)
            return bonus
        })(stream.FromSlice(dailySales)))

    if err != nil {
        fmt.Printf("âŒ Error calculating bonus total: %v\n", err)
        return
    }

    fmt.Printf("\nâœ… Total with 10%% bonuses: $%d\n", bonusTotal)
    fmt.Printf("ðŸ“ˆ That's $%d more than regular total!\n", bonusTotal-totalRevenue)
}
```

## How Aggregation Works

Unlike `Map` and `Where` which return streams, aggregation functions **consume** the entire stream:

```
Stream Processing:    [Data] â†’ Map â†’ Where â†’ [More Data]
Aggregation:         [Data] â†’ Sum â†’ Single Number
```

Here's what happens internally:

```go
// For Sum([10, 20, 30, 40, 50]):
result = 0
result = result + 10  // = 10
result = result + 20  // = 30
result = result + 30  // = 60
result = result + 40  // = 100
result = result + 50  // = 150
return 150
```

## Combining Aggregation with Pipelines

The real power comes from **aggregating after transformation**:

```go
// Question: What's the sum of all even numbers squared?
result := stream.Sum(
    stream.Map(func(x int64) int64 { return x * x })(      // Square each
        stream.Where(func(x int64) bool { return x%2 == 0 })( // Keep evens
            stream.FromSlice([]int64{1,2,3,4,5,6,7,8,9,10}))))

// Pipeline: [1,2,3,4,5,6,7,8,9,10] â†’ [2,4,6,8,10] â†’ [4,16,36,64,100] â†’ 220
```

## What You'll See

When you run the sales analysis code:
```
ðŸ“Š Learning Aggregation - Sales Data Analysis!
ðŸ“ˆ Daily sales this week: [1200 1800 950 2100 1650 2400 1900]

ðŸ§® Calculating business metrics...
   ðŸ’° Total weekly revenue: $12000
   ðŸ“… Number of business days: 7
   ðŸ“Š Average daily sales: $1714
   ðŸ† Best sales day: $2400
   ðŸ“‰ Lowest sales day: $950

ðŸ”„ Advanced: Aggregating after transformation...
   ðŸ’¡ $1200 + 10% bonus = $1320
   ðŸ’¡ $1800 + 10% bonus = $1980
   ðŸ’¡ $950 + 10% bonus = $1045
   ðŸ’¡ $2100 + 10% bonus = $2310
   ðŸ’¡ $1650 + 10% bonus = $1815
   ðŸ’¡ $2400 + 10% bonus = $2640
   ðŸ’¡ $1900 + 10% bonus = $2090

âœ… Total with 10% bonuses: $13200
ðŸ“ˆ That's $1200 more than regular total!
```

## Key Aggregation Insights

1. **Terminal Operations**: Aggregations end your stream pipeline
   - You can't chain more operations after aggregation
   - They return concrete values, not streams

2. **Type Safety**: Aggregations work with appropriate types
   - `Sum`, `Max`, `Min` need comparable/numeric types
   - `Count` works with any type

3. **Empty Streams**: Handle empty data gracefully
   - `Count` returns 0
   - Other aggregations may return errors

4. **Performance**: Aggregations are optimized
   - Single pass through your data
   - Memory efficient processing

## Common Aggregation Patterns

```go
// Business metrics
totalSales := stream.Sum(salesStream)
customerCount := stream.Count(customerStream)
averageOrderValue := stream.Avg(orderValueStream)

// Quality control
highestScore := stream.Max(testScoreStream)
lowestTemperature := stream.Min(temperatureStream)

// Data analysis
outlierCount := stream.Count(
    stream.Where(func(x float64) bool { return x > threshold })(dataStream))
```

You've now learned how to summarize and analyze your stream data! Next, we'll work with real-world data formats like CSV files.

---

# Step 6: Working with Real Data - CSV Processing

## From Spreadsheets to Streams

Now let's apply everything you've learned to **real-world data**! CSV (Comma-Separated Values) files are everywhere in business - from sales reports to customer databases to financial records.

## What Are CSV Files?

CSV files are like **digital spreadsheets** stored as plain text:

```
name,age,salary,department
Alice,30,75000,Engineering
Bob,25,65000,Sales
Charlie,35,85000,Engineering
```

Each row is a **record** (like a database row), and each column is a **field**. StreamV2 converts these into `Record` objects that you can process with all the operations you've learned!

## Understanding StreamV2 Records

When StreamV2 reads CSV data, it creates `Record` objects:

```go
// CSV row: "Alice,30,75000,Engineering"
// Becomes: stream.Record{
//   "name": "Alice",
//   "age": 30,
//   "salary": 75000,
//   "department": "Engineering"
// }
```

## Your First CSV Analysis

Let's analyze employee data from a company's HR system:

```go
package main

import (
    "fmt"
    "strings"
    "github.com/rosscartlidge/streamv2/pkg/stream"
)

func main() {
    fmt.Println("ðŸ“Š Employee Data Analysis with CSV!")

    // Sample employee data (in real life, this would come from a .csv file)
    csvData := `name,age,salary,department
Alice,30,75000,Engineering
Bob,25,65000,Sales
Charlie,35,85000,Engineering
Diana,28,70000,Marketing
Eve,32,80000,Engineering
Frank,29,72000,Sales
Grace,26,68000,Marketing`

    fmt.Println("ðŸ“‚ Loading employee data from CSV...")

    // Parse CSV data into Record objects
    records, err := stream.Collect(
        stream.CSVToStream(strings.NewReader(csvData)))

    if err != nil {
        fmt.Printf("âŒ Error reading CSV: %v\n", err)
        return
    }

    fmt.Printf("âœ… Loaded %d employee records\n", len(records))

    // Let's explore what a Record looks like
    fmt.Println("\nðŸ” Examining the first record:")
    firstEmployee := records[0]
    fmt.Printf("   Record structure: %+v\n", firstEmployee)
    fmt.Printf("   Name: %v (type: %T)\n", firstEmployee["name"], firstEmployee["name"])
    fmt.Printf("   Age: %v (type: %T)\n", firstEmployee["age"], firstEmployee["age"])

    // ANALYSIS 1: Extract all employee names
    fmt.Println("\nðŸ‘¥ Analysis 1: All Employee Names")
    names, err := stream.Collect(
        stream.Map(func(record stream.Record) string {
            // Extract the "name" field and convert to string
            name := stream.GetOr(record, "name", "Unknown")
            fmt.Printf("   ðŸ“ Extracted name: %s\n", name)
            return name
        })(stream.FromSlice(records)))

    if err != nil {
        fmt.Printf("âŒ Error extracting names: %v\n", err)
        return
    }

    fmt.Printf("âœ… All employees: %v\n", names)

    // ANALYSIS 2: Find all Engineering employees
    fmt.Println("\nðŸ”§ Analysis 2: Engineering Department")
    engineers, err := stream.Collect(
        stream.Where(func(record stream.Record) bool {
            dept := stream.GetOr(record, "department", "")
            isEngineer := dept == "Engineering"
            fmt.Printf("   ðŸ” %s works in %s â†’ Engineer? %v\n",
                      stream.GetOr(record, "name", "Unknown"), dept, isEngineer)
            return isEngineer
        })(stream.FromSlice(records)))

    if err != nil {
        fmt.Printf("âŒ Error filtering engineers: %v\n", err)
        return
    }

    fmt.Printf("\nâœ… Found %d engineers:\n", len(engineers))
    for _, eng := range engineers {
        name := stream.GetOr(eng, "name", "Unknown")
        age := stream.GetOr(eng, "age", 0)
        salary := stream.GetOr(eng, "salary", 0)
        fmt.Printf("   ðŸ‘¨â€ðŸ’» %s: age %v, salary $%v\n", name, age, salary)
    }

    // ANALYSIS 3: Calculate average engineering salary
    fmt.Println("\nðŸ’° Analysis 3: Engineering Salary Analysis")
    avgEngSalary, err := stream.Avg(
        stream.Map(func(record stream.Record) float64 {
            salary := stream.GetOr(record, "salary", float64(0))
            fmt.Printf("   ðŸ’µ Engineering salary: $%.0f\n", salary)
            return salary
        })(stream.FromSlice(engineers)))

    if err != nil {
        fmt.Printf("âŒ Error calculating average: %v\n", err)
        return
    }

    fmt.Printf("âœ… Average engineering salary: $%.0f\n", avgEngSalary)
}
```

## Understanding Record Access

StreamV2 provides safe ways to access Record fields:

### Method 1: Direct Access (Risky)
```go
name := record["name"].(string)  // Crashes if field missing or wrong type!
```

### Method 2: Safe Access (Recommended)
```go
name := stream.GetOr(record, "name", "Unknown")  // Returns default if missing
```

The `GetOr` function is your friend - it prevents crashes and provides sensible defaults!

## What You'll See

When you run the employee analysis:
```
ðŸ“Š Employee Data Analysis with CSV!
ðŸ“‚ Loading employee data from CSV...
âœ… Loaded 7 employee records

ðŸ” Examining the first record:
   Record structure: map[age:30 department:Engineering name:Alice salary:75000]
   Name: Alice (type: string)
   Age: 30 (type: int64)

ðŸ‘¥ Analysis 1: All Employee Names
   ðŸ“ Extracted name: Alice
   ðŸ“ Extracted name: Bob
   ðŸ“ Extracted name: Charlie
   ðŸ“ Extracted name: Diana
   ðŸ“ Extracted name: Eve
   ðŸ“ Extracted name: Frank
   ðŸ“ Extracted name: Grace
âœ… All employees: [Alice Bob Charlie Diana Eve Frank Grace]

ðŸ”§ Analysis 2: Engineering Department
   ðŸ” Alice works in Engineering â†’ Engineer? true
   ðŸ” Bob works in Sales â†’ Engineer? false
   ðŸ” Charlie works in Engineering â†’ Engineer? true
   ðŸ” Diana works in Marketing â†’ Engineer? false
   ðŸ” Eve works in Engineering â†’ Engineer? true
   ðŸ” Frank works in Sales â†’ Engineer? false
   ðŸ” Grace works in Marketing â†’ Engineer? false

âœ… Found 3 engineers:
   ðŸ‘¨â€ðŸ’» Alice: age 30, salary $75000
   ðŸ‘¨â€ðŸ’» Charlie: age 35, salary $85000
   ðŸ‘¨â€ðŸ’» Eve: age 32, salary $80000

ðŸ’° Analysis 3: Engineering Salary Analysis
   ðŸ’µ Engineering salary: $75000
   ðŸ’µ Engineering salary: $85000
   ðŸ’µ Engineering salary: $80000
âœ… Average engineering salary: $80000
```

## Key CSV Processing Concepts

1. **Automatic Parsing**: `CSVToStream` handles the parsing for you
   - Headers become field names
   - Data gets appropriate Go types (string, int64, float64)

2. **Record Structure**: Each CSV row becomes a `Record`
   - Think of it as `map[string]any`
   - Access fields by column name: `record["salary"]`

3. **Type Safety**: Use `GetOr` for safe field access
   - Prevents crashes from missing fields
   - Provides sensible defaults

4. **Real Data**: CSV processing opens the door to real-world applications
   - Sales reports, customer databases, sensor data
   - Any spreadsheet can become a stream!

## Reading From Actual Files

In real applications, you'd read from files:

```go
// Read from a real CSV file
file, _ := os.Open("employees.csv")
defer file.Close()

records, _ := stream.Collect(stream.CSVToStream(file))
```

You've just learned how to process real-world data with StreamV2! Next, we'll build more sophisticated analysis pipelines.

---

# Step 7: Data Analysis Pipeline

## Building Professional Analytics

Now let's combine everything you've learned into a **complete data analysis pipeline**! This is where StreamV2 really shines - building sophisticated analytics that would take dozens of lines in traditional Go.

## Real-World Scenario: HR Analytics

Imagine you're a data analyst at a tech company. HR wants insights about:
- ðŸ’° Average salaries by department
- ðŸ† High-performing engineers (experience + salary)
- ðŸ“Š Company-wide salary distribution
- ðŸŽ¯ Which departments to focus recruitment on

Let's build this analysis step by step!

```go
package main

import (
    "fmt"
    "strings"
    "github.com/rosscartlidge/streamv2/pkg/stream"
)

func main() {
    csvData := `name,age,salary,department,years_experience
Alice,30,75000,Engineering,5
Bob,25,65000,Sales,3
Charlie,35,85000,Engineering,10
Diana,28,70000,Marketing,4
Eve,32,80000,Engineering,7
Frank,29,68000,Sales,4
Grace,33,82000,Engineering,8
Henry,26,62000,Marketing,2
Iris,31,77000,Engineering,6
Jack,27,64000,Sales,3`

    records, err := stream.Collect(
        stream.CSVToStream(strings.NewReader(csvData)))
    if err != nil {
        panic(err)
    }

    fmt.Printf("=== Employee Data Analysis ===\n")
    fmt.Printf("Total employees: %d\n\n", len(records))

    // 1. Average salary by department
    depts := []string{"Engineering", "Sales", "Marketing"}
    
    for _, dept := range depts {
        deptEmployees, _ := stream.Collect(
            stream.Where(func(r stream.Record) bool {
                return r["department"].(string) == dept
            })(stream.FromSlice(records)))
        
        avgSalary, _ := stream.Avg(
            stream.Map(func(r stream.Record) float64 {
                return float64(r["salary"].(int64))
            })(stream.FromSlice(deptEmployees)))
        
        fmt.Printf("%s: %d employees, avg salary: $%.0f\n", 
                   dept, len(deptEmployees), avgSalary)
    }
    
    fmt.Println()

    // 2. High performers: Engineers with >6 years experience and >$75k salary
    highPerformers, _ := stream.Collect(
        stream.Pipe(
            stream.Where(func(r stream.Record) bool {
                return r["department"].(string) == "Engineering"
            }),
            stream.Where(func(r stream.Record) bool {
                return r["years_experience"].(int64) > 6
            }),
            stream.Where(func(r stream.Record) bool {
                return r["salary"].(int64) > 75000
            }),
        )(stream.FromSlice(records)))

    fmt.Printf("High-performing engineers (>6 years, >$75k): %d\n", len(highPerformers))
    for _, hp := range highPerformers {
        fmt.Printf("- %s: %d years, $%d\n", 
                   hp["name"], hp["years_experience"], hp["salary"])
    }
    
    fmt.Println()

    // 3. Salary efficiency: salary per year of experience
    efficiencyData, _ := stream.Collect(
        stream.Map(func(r stream.Record) map[string]any {
            salary := float64(r["salary"].(int64))
            experience := float64(r["years_experience"].(int64))
            efficiency := salary / experience
            
            return map[string]any{
                "name": r["name"],
                "department": r["department"],
                "salary_per_year": efficiency,
            }
        })(stream.FromSlice(records)))

    // Find most efficient employee
    maxEfficiency, _ := stream.Max(
        stream.Map(func(r map[string]any) float64 {
            return r["salary_per_year"].(float64)
        })(stream.FromSlice(efficiencyData)))

    mostEfficient, _ := stream.Collect(
        stream.Where(func(r map[string]any) bool {
            return r["salary_per_year"].(float64) == maxEfficiency
        })(stream.FromSlice(efficiencyData)))

    fmt.Printf("Most salary-efficient employee:\n")
    emp := mostEfficient[0]
    fmt.Printf("- %s (%s): $%.0f per year of experience\n", 
               emp["name"], emp["department"], emp["salary_per_year"])
}
```

**Output:**
```
=== Employee Data Analysis ===
Total employees: 10

Engineering: 5 employees, avg salary: $77800
Sales: 3 employees, avg salary: $65667
Marketing: 2 employees, avg salary: $66000

High-performing engineers (>6 years, >$75k): 3
- Charlie: 10 years, $85000
- Eve: 7 years, $80000
- Grace: 8 years, $82000

Most salary-efficient employee:
- Henry (Marketing): $31000 per year of experience
```

**Key Concepts:**
- Complex pipelines combining multiple Where clauses
- Data transformation creating new calculated fields
- Multi-step analysis using intermediate results
- Type conversions for mathematical operations

---

# Step 8: Working with JSON Data

Let's process JSON data, which is common in modern applications.

```go
package main

import (
    "fmt"
    "strings"
    "github.com/rosscartlidge/streamv2/pkg/stream"
)

func main() {
    // JSON Lines format (one JSON object per line)
    jsonData := `{"product": "Widget A", "price": 19.99, "category": "Tools", "units_sold": 150}
{"product": "Gadget B", "price": 29.99, "category": "Electronics", "units_sold": 89}
{"product": "Tool C", "price": 39.99, "category": "Tools", "units_sold": 67}
{"product": "Device D", "price": 49.99, "category": "Electronics", "units_sold": 203}
{"product": "Widget E", "price": 24.99, "category": "Tools", "units_sold": 124}`

    // Parse JSON data
    products, err := stream.Collect(
        stream.JSONToStream(strings.NewReader(jsonData)))
    if err != nil {
        panic(err)
    }

    fmt.Printf("=== Product Sales Analysis ===\n")
    fmt.Printf("Total products: %d\n\n", len(products))

    // 1. Calculate total revenue per product
    revenues, err := stream.Collect(
        stream.Map(func(p stream.Record) map[string]any {
            price := p["price"].(float64)
            units := p["units_sold"].(int64)
            revenue := price * float64(units)
            
            return map[string]any{
                "product": p["product"],
                "category": p["category"],
                "revenue": revenue,
                "units_sold": units,
            }
        })(stream.FromSlice(products)))

    if err != nil {
        panic(err)
    }

    // 2. Find best-selling product by revenue
    maxRevenue, _ := stream.Max(
        stream.Map(func(p map[string]any) float64 {
            return p["revenue"].(float64)
        })(stream.FromSlice(revenues)))

    bestSeller, _ := stream.Collect(
        stream.Where(func(p map[string]any) bool {
            return p["revenue"].(float64) == maxRevenue
        })(stream.FromSlice(revenues)))

    fmt.Printf("Best-selling product by revenue:\n")
    top := bestSeller[0]
    fmt.Printf("- %s: $%.2f revenue (%d units)\n\n", 
               top["product"], top["revenue"], top["units_sold"])

    // 3. Category analysis with GroupBy
    // Convert to Records for structured data processing
    recordStream := stream.Map(func(p map[string]any) stream.Record {
        return stream.Record(p)  // Convert map to Record
    })(stream.FromSlice(revenues))

    // Group by category and calculate aggregations
    categoryStats, _ := stream.Collect(
        stream.GroupBy([]string{"category"}, 
            stream.SumField[float64]("total_revenue", "revenue"),
            stream.SumField[int64]("total_units", "units_sold"),
            stream.CountField("product_count", "product"),
        )(recordStream))

    fmt.Printf("Revenue by category:\n")
    for _, stat := range categoryStats {
        category := stream.GetOr(stat, "category", "")
        revenue := stream.GetOr(stat, "total_revenue", 0.0)
        units := stream.GetOr(stat, "total_units", int64(0))
        products := stream.GetOr(stat, "product_count", int64(0))
        
        fmt.Printf("- %s: $%.2f revenue, %d units sold (%d products)\n", 
                   category, revenue, units, products)
    }

    // 4. Export results back to JSON
    var jsonOutput strings.Builder
    err = stream.StreamToJSON(stream.FromSlice(revenues), &jsonOutput)
    if err != nil {
        panic(err)
    }

    fmt.Printf("\n=== Exported JSON (first 200 chars) ===\n")
    output := jsonOutput.String()
    if len(output) > 200 {
        fmt.Printf("%s...\n", output[:200])
    } else {
        fmt.Println(output)
    }
}
```

**Output:**
```
=== Product Sales Analysis ===
Total products: 5

Best-selling product by revenue:
- Device D: $10147.97 revenue (203 units)

Revenue by category:
- Tools: $8646.33 revenue, 341 units sold (3 products)
- Electronics: $12046.08 revenue, 292 units sold (2 products)

=== Exported JSON (first 200 chars) ===
{"category":"Tools","product":"Widget A","revenue":2998.5,"units_sold":150}
{"category":"Electronics","product":"Gadget B","revenue":2669.11,"units_sold":89}
{"ca...
```

**Key Concepts:**
- `JSONToStream()` and `StreamToJSON()` for JSON processing
- JSON numbers become float64, integers become int64
- `GroupBy()` with explicit aggregations (no automatic count)
- Modern aggregation functions: `SumField`, `CountField`, etc.
- Type-safe record access with `GetOr()`
- Round-trip data processing (JSON â†’ analysis â†’ JSON)

---

# Step 9: Join Operations - Combining Data Streams

Real-world data often comes from multiple sources. StreamV2 provides SQL-style join operations to combine data streams efficiently.

```go
package main

import (
    "fmt"
    "github.com/rosscartlidge/streamv2/pkg/stream"
)

func main() {
    fmt.Printf("=== Join Operations ===\n\n")

    // Create user data
    users := []stream.Record{
        stream.NewRecord().Int("id", 1).String("name", "Alice").String("email", "alice@example.com").Build(),
        stream.NewRecord().Int("id", 2).String("name", "Bob").String("email", "bob@example.com").Build(),
        stream.NewRecord().Int("id", 3).String("name", "Charlie").String("email", "charlie@example.com").Build(),
    }

    // Create profile data (notice: not all users have profiles)
    profiles := []stream.Record{
        stream.NewRecord().Int("userId", 1).String("department", "Engineering").Int("salary", 75000).Build(),
        stream.NewRecord().Int("userId", 2).String("department", "Sales").Int("salary", 65000).Build(),
        // Charlie (id=3) has no profile
    }

    userStream := stream.FromSlice(users)
    profileStream := stream.FromSlice(profiles)

    fmt.Printf("Users: %d records\n", len(users))
    fmt.Printf("Profiles: %d records\n\n", len(profiles))

    // 1. Inner Join - Only users who have profiles
    fmt.Printf("=== Inner Join (users with profiles) ===\n")
    innerJoined, _ := stream.Collect(
        stream.InnerJoin(profileStream, "id", "userId")(userStream))

    for _, record := range innerJoined {
        name := stream.GetOr(record, "name", "")
        dept := stream.GetOr(record, "department", "")
        salary := stream.GetOr(record, "salary", int64(0))
        fmt.Printf("- %s: %s, $%d\n", name, dept, salary)
    }
    fmt.Printf("Results: %d records\n\n", len(innerJoined))

    // 2. Left Join - All users, with profile data when available
    fmt.Printf("=== Left Join (all users, profiles when available) ===\n")

    // Need fresh streams for the next join
    userStream2 := stream.FromSlice(users)
    profileStream2 := stream.FromSlice(profiles)

    leftJoined, _ := stream.Collect(
        stream.LeftJoin(profileStream2, "id", "userId")(userStream2))

    for _, record := range leftJoined {
        name := stream.GetOr(record, "name", "")
        dept := stream.GetOr(record, "department", "Unknown")
        salary := stream.GetOr(record, "salary", int64(0))

        if salary > 0 {
            fmt.Printf("- %s: %s, $%d\n", name, dept, salary)
        } else {
            fmt.Printf("- %s: %s (no profile)\n", name, dept)
        }
    }
    fmt.Printf("Results: %d records\n\n", len(leftJoined))

    // 3. Join with custom prefixes to handle field conflicts
    fmt.Printf("=== Custom Prefixes (avoiding field conflicts) ===\n")

    // Create data with conflicting field names
    employees := []stream.Record{
        stream.NewRecord().Int("id", 1).String("name", "Alice").Build(),
        stream.NewRecord().Int("id", 2).String("name", "Bob").Build(),
    }

    managers := []stream.Record{
        stream.NewRecord().Int("id", 1).String("name", "Alice Manager").String("title", "VP Engineering").Build(),
        stream.NewRecord().Int("id", 2).String("name", "Bob Manager").String("title", "Sales Director").Build(),
    }

    empStream := stream.FromSlice(employees)
    mgrStream := stream.FromSlice(managers)

    customJoined, _ := stream.Collect(
        stream.InnerJoin(mgrStream, "id", "id",
            stream.WithPrefixes("emp.", "mgr."))(empStream))

    for _, record := range customJoined {
        empName := stream.GetOr(record, "emp.name", "")
        mgrName := stream.GetOr(record, "mgr.name", "")
        title := stream.GetOr(record, "title", "")
        fmt.Printf("- Employee: %s, Manager: %s (%s)\n", empName, mgrName, title)
    }
}
```

**What you learned:**
- `InnerJoin()` returns only records that match in both streams
- `LeftJoin()` returns all records from the left stream, with right data when available
- Join operations work on key fields (converted to strings for comparison)
- `WithPrefixes()` handles field name conflicts with custom prefixes
- Right stream is loaded into memory (must be finite and reasonably sized)
- StreamV2 uses efficient hash join algorithms for O(n + m) performance

**Real-world use cases:**
- Combining user data with profiles/preferences
- Joining order data with customer information
- Merging sales data with product catalogs
- Associating log entries with user sessions

---

# Step 10: Sorting Your Data

Sometimes you need your data in a specific order. StreamV2 provides flexible sorting capabilities.

```go
package main

import (
    "fmt"
    "github.com/rosscartlidge/streamv2/pkg/stream"
)

func main() {
    // Sample student scores
    scores := []int{85, 92, 78, 96, 81, 88, 94, 76, 90, 87}

    fmt.Printf("=== Sorting Operations ===\n")
    fmt.Printf("Original scores: %v\n\n", scores)

    // Sort in ascending order
    ascending, _ := stream.Collect(
        stream.SortAsc[int]()(
            stream.FromSlice(scores)))
    fmt.Printf("Ascending order: %v\n", ascending)

    // Sort in descending order
    descending, _ := stream.Collect(
        stream.SortDesc[int]()(
            stream.FromSlice(scores)))
    fmt.Printf("Descending order: %v\n\n", descending)

    // Sorting student records by multiple fields
    students := []stream.Record{
        stream.NewRecord().String("name", "Alice").Int("grade", 85).String("subject", "Math").Build(),
        stream.NewRecord().String("name", "Bob").Int("grade", 92).String("subject", "Science").Build(),
        stream.NewRecord().String("name", "Alice").Int("grade", 78).String("subject", "Science").Build(),
        stream.NewRecord().String("name", "Charlie").Int("grade", 96).String("subject", "Math").Build(),
    }

    // Sort by name, then by grade
    sorted, _ := stream.Collect(
        stream.SortBy("name", "grade")(
            stream.FromSlice(students)))

    fmt.Printf("Students sorted by name, then grade:\n")
    for _, student := range sorted {
        name := stream.GetOr(student, "name", "")
        grade := stream.GetOr(student, "grade", int64(0))
        subject := stream.GetOr(student, "subject", "")
        fmt.Printf("- %s: %d (%s)\n", name, grade, subject)
    }
}
```

**What you learned:**
- `SortAsc()` and `SortDesc()` for simple ascending/descending sorts
- `SortBy(fields...)` for sorting records by multiple fields
- Sorting works with any comparable data type
- Multiple sort criteria are applied in order (name first, then grade)

**Important:** Basic sorting loads the entire stream into memory, so it's designed for finite data sets. For infinite streams, you'll need windowed sorting (covered in the advanced codelab).

---

# Step 11: Advanced Stream Operations

Let's explore some more advanced stream operations for powerful data processing.

```go
package main

import (
    "fmt"
    "github.com/rosscartlidge/streamv2/pkg/stream"
)

func main() {
    // Sample transaction data
    transactions := []map[string]any{
        {"id": 1, "amount": 100.50, "type": "credit", "user_id": "u1"},
        {"id": 2, "amount": 50.25, "type": "debit", "user_id": "u1"},
        {"id": 3, "amount": 200.00, "type": "credit", "user_id": "u2"},
        {"id": 4, "amount": 75.00, "type": "debit", "user_id": "u2"},
        {"id": 5, "amount": 300.00, "type": "credit", "user_id": "u1"},
        {"id": 6, "amount": 25.75, "type": "debit", "user_id": "u3"},
    }

    fmt.Printf("=== Advanced Stream Operations ===\n")
    fmt.Printf("Total transactions: %d\n\n", len(transactions))

    // 1. Take and Skip operations
    firstThree, _ := stream.Collect(
        stream.Take[map[string]any](3)(
            stream.FromSlice(transactions)))

    fmt.Printf("First 3 transactions:\n")
    for _, tx := range firstThree {
        fmt.Printf("- ID %v: $%.2f %s\n", tx["id"], tx["amount"], tx["type"])
    }
    fmt.Println()

    skipTwo, _ := stream.Collect(
        stream.Skip[map[string]any](2)(
            stream.FromSlice(transactions)))

    fmt.Printf("After skipping first 2 transactions: %d remaining\n\n", len(skipTwo))

    // 2. Multiple aggregations at once
    creditAmounts, _ := stream.Collect(
        stream.Map(func(tx map[string]any) float64 {
            if tx["type"].(string) == "credit" {
                return tx["amount"].(float64)
            }
            return 0
        })(stream.FromSlice(transactions)))

    debitAmounts, _ := stream.Collect(
        stream.Map(func(tx map[string]any) float64 {
            if tx["type"].(string) == "debit" {
                return tx["amount"].(float64)
            }
            return 0
        })(stream.FromSlice(transactions)))

    totalCredits, _ := stream.Sum(stream.FromSlice(creditAmounts))
    totalDebits, _ := stream.Sum(stream.FromSlice(debitAmounts))
    netBalance := totalCredits - totalDebits

    fmt.Printf("Financial Summary:\n")
    fmt.Printf("- Total Credits: $%.2f\n", totalCredits)
    fmt.Printf("- Total Debits:  $%.2f\n", totalDebits)
    fmt.Printf("- Net Balance:   $%.2f\n\n", netBalance)

    // 3. Complex filtering and transformation
    largeCredits, _ := stream.Collect(
        stream.Pipe(
            stream.Where(func(tx map[string]any) bool {
                return tx["type"].(string) == "credit"
            }),
            stream.Where(func(tx map[string]any) bool {
                return tx["amount"].(float64) >= 200.0
            }),
            stream.Map(func(tx map[string]any) string {
                return fmt.Sprintf("User %s received $%.2f", 
                                   tx["user_id"], tx["amount"])
            }),
        )(stream.FromSlice(transactions)))

    fmt.Printf("Large credit transactions (>= $200):\n")
    for _, msg := range largeCredits {
        fmt.Printf("- %s\n", msg)
    }
    fmt.Println()

    // 4. User account balances
    users := []string{"u1", "u2", "u3"}
    
    fmt.Printf("User Account Balances:\n")
    for _, userID := range users {
        userTxs, _ := stream.Collect(
            stream.Where(func(tx map[string]any) bool {
                return tx["user_id"].(string) == userID
            })(stream.FromSlice(transactions)))

        balance := 0.0
        for _, tx := range userTxs {
            amount := tx["amount"].(float64)
            if tx["type"].(string) == "credit" {
                balance += amount
            } else {
                balance -= amount
            }
        }

        fmt.Printf("- User %s: $%.2f (%d transactions)\n", 
                   userID, balance, len(userTxs))
    }
}
```

**Output:**
```
=== Advanced Stream Operations ===
Total transactions: 6

First 3 transactions:
- ID 1: $100.50 credit
- ID 2: $50.25 debit
- ID 3: $200.00 credit

After skipping first 2 transactions: 4 remaining

Financial Summary:
- Total Credits: $600.50
- Total Debits:  $151.00
- Net Balance:   $449.50

Large credit transactions (>= $200):
- User u2 received $200.00
- User u1 received $300.00

User Account Balances:
- User u1: $350.25 (3 transactions)
- User u2: $125.00 (2 transactions)
- User u3: $-25.75 (1 transactions)
```

**Key Concepts:**
- `Take()` and `Skip()` for pagination and limiting results
- Multiple aggregations using separate pipelines
- Complex multi-step filtering and transformation
- Business logic implementation using stream operations

---

# Step 12: Best Practices and Performance Tips

## Writing Production-Ready Stream Code

Congratulations! You've learned all the core StreamV2 operations. Now let's cover the **best practices** that will help you write clean, efficient, and maintainable stream processing code.

## 1. Always Handle Errors Properly

StreamV2 operations can fail, especially when reading files or processing invalid data. **Always check errors**:

```go
// âŒ BAD: Ignoring errors
result, _ := stream.Sum(stream.FromSlice(numbers))

// âœ… GOOD: Proper error handling
result, err := stream.Sum(stream.FromSlice(numbers))
if err != nil {
    log.Printf("Failed to calculate sum: %v", err)
    return
}
```

**Why this matters**: In production, ignoring errors leads to silent failures and debugging nightmares.

## 2. Use Pipe for Readable Pipelines

When chaining multiple operations, use `Pipe` instead of nested function calls:

```go
// âŒ BAD: Hard to read (right-to-left)
result := stream.Where(func(x int) bool { return x > 5 })(
    stream.Map(func(x int) int { return x * x })(
        stream.FromSlice(data)))

// âœ… GOOD: Easy to read (left-to-right)
result := stream.Pipe(
    stream.Map(func(x int) int { return x * x }),
    stream.Where(func(x int) bool { return x > 5 }),
)(stream.FromSlice(data))
```

**Why this matters**: Your future self (and your teammates) will thank you for readable code.

## 3. Use Multiple Aggregations for Efficiency

When you need several statistics, compute them in one pass:

```go
// âŒ BAD: Multiple passes through data
sum, _ := stream.Sum(stream.FromSlice(numbers))
count, _ := stream.Count(stream.FromSlice(numbers))  // Reads data again!
avg, _ := stream.Avg(stream.FromSlice(numbers))      // Reads data again!

// âœ… GOOD: Single pass with multiple aggregations
stats, err := stream.Aggregates(stream.FromSlice(numbers),
    stream.SumStream[int64]("total"),
    stream.CountStream[int64]("count"),
    stream.AvgStream[int64]("average"),
)
```

**Why this matters**: Reading data once is much faster than reading it multiple times.

## 4. Use GetOr for Safe Record Access

When working with Records (from CSV, JSON), always use safe accessors:

```go
// âŒ BAD: Can crash if field missing
name := record["name"].(string)
salary := record["salary"].(int64)

// âœ… GOOD: Safe with sensible defaults
name := stream.GetOr(record, "name", "Unknown")
salary := stream.GetOr(record, "salary", int64(0))
```

**Why this matters**: Real-world data is messy. Missing fields shouldn't crash your program.

## 5. Name Complex Operations

For complex transformations, create named functions:

```go
// âŒ BAD: Inline complexity
stream.Map(func(r stream.Record) float64 {
    base := stream.GetOr(r, "salary", 0.0)
    bonus := stream.GetOr(r, "bonus", 0.0)
    return (base + bonus) * 1.15 // What does 1.15 mean?
})(records)

// âœ… GOOD: Named functions with clear intent
calculateTotalCompensation := func(r stream.Record) float64 {
    baseSalary := stream.GetOr(r, "salary", 0.0)
    bonusAmount := stream.GetOr(r, "bonus", 0.0)
    const TAX_MULTIPLIER = 1.15
    return (baseSalary + bonusAmount) * TAX_MULTIPLIER
}

stream.Map(calculateTotalCompensation)(records)
```

**Why this matters**: Self-documenting code reduces bugs and improves maintainability.

## 6. Test Your Stream Pipelines

Write tests for your stream processing logic:

```go
func TestSalaryAnalysis(t *testing.T) {
    // Arrange: Create test data
    testData := []stream.Record{
        stream.NewRecord().String("name", "Alice").Int("salary", 75000).Build(),
        stream.NewRecord().String("name", "Bob").Int("salary", 65000).Build(),
    }

    // Act: Run your stream pipeline
    avgSalary, err := stream.Avg(
        stream.Map(func(r stream.Record) float64 {
            return float64(stream.GetOr(r, "salary", 0))
        })(stream.FromSlice(testData)))

    // Assert: Verify results
    assert.NoError(t, err)
    assert.Equal(t, 70000.0, avgSalary)
}
```

**Why this matters**: Stream pipelines can be complex. Tests ensure they work correctly.

## 7. Performance Guidelines

### For Small Data (< 10,000 records)
- Use simple operations
- Don't worry about optimization
- Focus on code clarity

### For Large Data (> 100,000 records)
- Consider processing in chunks
- Use memory-efficient operations
- Monitor performance

```go
// Example: Processing large datasets efficiently
func processLargeDataset(data []stream.Record) {
    const CHUNK_SIZE = 10000

    for i := 0; i < len(data); i += CHUNK_SIZE {
        end := i + CHUNK_SIZE
        if end > len(data) {
            end = len(data)
        }

        chunk := data[i:end]
        result := stream.Map(expensiveTransformation)(
            stream.FromSlice(chunk))

        // Process chunk result...
    }
}
```

## 8. Common Patterns to Remember

### Data Cleaning
```go
cleanData := stream.Pipe(
    stream.Where(isValidRecord),      // Remove bad data
    stream.Map(normalizeFields),      // Standardize formats
    stream.Where(hasRequiredFields),  // Final validation
)
```

### Analysis Pipeline
```go
analysis := stream.Pipe(
    stream.Where(matchesCriteria),    // Filter relevant data
    stream.Map(extractMetrics),       // Transform for analysis
    // Then aggregate or collect
)
```

### Data Export
```go
export := stream.Pipe(
    stream.Map(formatForExport),      // Prepare for output format
    stream.Where(includeInExport),    // Final filtering
    // Then write to file/database
)
```

## Summary: Your StreamV2 Journey

You've learned how to:
- âœ… **Transform** data with Map
- âœ… **Filter** data with Where
- âœ… **Combine** operations with Pipe
- âœ… **Aggregate** data for insights
- âœ… **Process** real-world CSV and JSON
- âœ… **Build** sophisticated analysis pipelines
- âœ… **Write** production-ready code

### Key Principles to Remember:
1. **Start Simple**: Begin with basic operations, add complexity gradually
2. **Error Handling**: Always check errors in production code
3. **Readability**: Use Pipe and named functions for clarity
4. **Testing**: Write tests for your stream processing logic
5. **Performance**: Optimize when needed, but clarity first

You're now ready to build powerful data processing applications with StreamV2!

    // 3. Memory efficiency - process in chunks for large data
    largeData := make([]int64, 10000)
    for i := range largeData {
        largeData[i] = int64(i + 1)
    }
    
    start := time.Now()
    
    // Process in smaller chunks rather than all at once
    chunkSize := 1000
    totalSum := int64(0)
    
    for i := 0; i < len(largeData); i += chunkSize {
        end := i + chunkSize
        if end > len(largeData) {
            end = len(largeData)
        }
        
        chunk := largeData[i:end]
        chunkSum, _ := stream.Sum(
            stream.Map(func(x int64) int64 { return x * x })(
                stream.FromSlice(chunk)))
        
        totalSum += chunkSum
    }
    
    duration := time.Since(start)
    fmt.Printf("âœ… Chunked processing of %d items: sum=%d in %v\n", 
               len(largeData), totalSum, duration)

    // 4. Type safety tips
    records := []stream.Record{
        {"name": "Alice", "age": 30, "salary": 75000},
        {"name": "Bob", "age": 25, "salary": 65000},
    }
    
    // Always use type assertions safely
    names, _ := stream.Collect(
        stream.Map(func(r stream.Record) string {
            // Safe type assertion with check
            if name, ok := r["name"].(string); ok {
                return name
            }
            return "Unknown"
        })(stream.FromSlice(records)))
    
    fmt.Printf("âœ… Safe type assertions: %v\n", names)

    // 5. Reusable stream operations
    // Define reusable operations as variables
    doubleNumbers := stream.Map(func(x int64) int64 { return x * 2 })
    onlyEvens := stream.Where(func(x int64) bool { return x%2 == 0 })
    
    // Combine reusable operations
    processed, _ := stream.Collect(
        stream.Pipe(doubleNumbers, onlyEvens)(
            stream.FromSlice([]int64{1, 2, 3, 4, 5})))
    
    fmt.Printf("âœ… Reusable operations: %v\n", processed)

    // 6. Performance measurement
    fmt.Printf("\n=== Performance Tips ===\n")
    
    // Measure operation performance
    testData := make([]int64, 1000)
    for i := range testData {
        testData[i] = int64(i + 1)
    }
    
    start = time.Now()
    simpleSum, _ := stream.Sum(stream.FromSlice(testData))
    simpleDuration := time.Since(start)
    
    start = time.Now()
    complexSum, _ := stream.Sum(
        stream.Map(func(x int64) int64 { return x })(  // Identity map
            stream.FromSlice(testData)))
    complexDuration := time.Since(start)
    
    fmt.Printf("Simple sum: %d in %v\n", simpleSum, simpleDuration)
    fmt.Printf("With map:   %d in %v\n", complexSum, complexDuration)
    fmt.Printf("Overhead:   %v (%.1fx slower)\n", 
               complexDuration-simpleDuration,
               float64(complexDuration)/float64(simpleDuration))

    fmt.Printf("\n=== Summary ===\n")
    fmt.Printf("âœ… Always handle errors\n")
    fmt.Printf("âœ… Use Pipe() for readable pipelines\n")
    fmt.Printf("âœ… Process large data in chunks\n")
    fmt.Printf("âœ… Use safe type assertions\n")
    fmt.Printf("âœ… Create reusable operations\n")
    fmt.Printf("âœ… Measure performance when needed\n")
    fmt.Printf("\nHappy streaming! ðŸš€\n")
}
```

**Key Takeaways:**
- Always check errors from stream operations
- Use `Pipe()` for readable multi-step operations
- Process large datasets in chunks to manage memory
- Use safe type assertions with records
- Create reusable stream operations as variables
- Measure performance for critical paths

---

# What's Next?

Congratulations! You've learned the fundamentals of stream processing with StreamV2. You can now:

- âœ… Create and transform streams
- âœ… Filter and aggregate data
- âœ… Process CSV and JSON files
- âœ… Build complex data analysis pipelines
- âœ… Apply best practices for performance and maintainability

## Further Exploration

- **ðŸ• [Advanced Windowing Codelab](ADVANCED_WINDOWING_CODELAB.md)**: Master event-time processing, session windows, watermarks, and real-time analytics
- **Parallel processing**: Automatic parallelization for large datasets
- **Custom aggregators**: Build your own aggregation functions
- **Integration patterns**: Connect streams to databases, APIs, and message queues

## Resources

- [API Documentation](api.md)
- [Performance Guide](../design_docs/performance.md)
- [Examples Repository](../stream_examples/)
- [Contributing Guide](../CONTRIBUTING.md)

Start building amazing data processing applications with StreamV2! ðŸŽ‰